{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c995bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdd5d310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl (45.7 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from en-core-web-md==3.2.0) (3.2.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (21.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: click<8.1.0 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in d:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from click<8.1.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d2ba844",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3267cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"wiki1.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7707c5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America(U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text) # tokenizing \n",
    "sen1 = list(doc.sents)[0]\n",
    "print(sen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d3f3302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['country', 'COUNTRY', 'NATION', 'nation', 'COUNTIRES', 'nations', 'member-states', 'worLd', 'World', 'world']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# word vector similarity, not sysnonym similarity\n",
    "your_country = \"country\"\n",
    "ms = nlp.vocab.vectors.most_similar(np.asarray([nlp.vocab.vectors[nlp.vocab.strings[your_country]]]), n=10)\n",
    "words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "distances = ms[2]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1126556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining degree of similarity\n",
    "doc1 = nlp(\"i like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "220adaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like salty fries and hamburgers. <-> Fast food tastes very good. 0.7799485285662074\n"
     ]
    }
   ],
   "source": [
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8769bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like salty fries and hamburgers. <-> Victoria is in Kolkata. 0.5086395749623951\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(\"Victoria is in Kolkata.\")\n",
    "# matching happens judging the overlapping, word clustering\n",
    "print(doc1, \"<->\", doc3, doc1.similarity(doc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5dc913c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(\"I enjoy oranges.\")\n",
    "doc5 = nlp(\"I enjoy apples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86cf6da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy oranges. ,-> I enjoy apples. 0.9607558420297302\n"
     ]
    }
   ],
   "source": [
    "# similar sentesnses, with similar clustering, overlapping\n",
    "# similar in terms of class, usage etc. (both are fruits and edible)\n",
    "print(doc4, \",->\", doc5, doc4.similarity(doc5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b4a7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc6 = nlp(\"i enjoy burgers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a56309d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy oranges. <-> i enjoy burgers. 0.8755329425467893\n"
     ]
    }
   ],
   "source": [
    "print(doc4, \"<->\", doc6, doc4.similarity(doc6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6c2bb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salty fries <-> hamburgers \n",
      " 0.730462372303009\n"
     ]
    }
   ],
   "source": [
    "# not much of overlapping (like both are food, but of disimilar categories)\n",
    "food1 = doc1[2:4]\n",
    "food2 = doc1[5]\n",
    "print(food1, \"<->\", food2,\"\\n\",food1.similarity(food2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ff89991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines in spacy \n",
    "# A pipeline is a sequence of pipes, or actors on data, that make alterations to the data or extract information from it.\n",
    "# making a blank pipeline\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20c6d353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x28d1d4cc6c0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline added\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e5b3bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "s = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\")\n",
    "soup = BeautifulSoup(s.content).text.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
    "nlp.max_length = 5278439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eccc2c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94133\n",
      "Wall time: 7.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc = nlp(soup)\n",
    "print (len(list(doc.sents)))\n",
    "# 94k sentences found, in 7.76 seconds, using sentencizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba928773",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "nlp2.max_length = 5278439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8559944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "doc = nlp2(soup)\n",
    "print (len(list(doc.sents)))\n",
    "# 112k sentences found, in 47 minutes, using the smallest model\n",
    "# the sentencizer doesn't check all the bounderies, \n",
    "# but the 'small model', checks all the bounderies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bed0ac10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'sentencizer': {'assigns': ['token.is_sent_start', 'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['sents_f', 'sents_p', 'sents_r'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'sentencizer': []},\n",
       " 'attrs': {'doc.sents': {'assigns': ['sentencizer'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['sentencizer'], 'requires': []}}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9160c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "27e31857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []}}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.analyze_pipes()\n",
    "# for all the models (small, medium and large) the analyzation\n",
    "# is same \n",
    "# this proves that any model like this is much more\n",
    "# effcient that the 'sentencizer' alone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f092e79e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
